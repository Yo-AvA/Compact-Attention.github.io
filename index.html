<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Compact Attention</title>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');

    document.addEventListener('DOMContentLoaded', function () {
      const toggleButton = document.getElementById('toggleButton');
      const tocContent = document.getElementById('tocContent');
      const tocHeader = document.querySelector('.toc-header');

      function toggleMenu() {
        tocContent.classList.toggle('collapsed');
        toggleButton.textContent = tocContent.classList.contains('collapsed') ? '▶' : '▼';
      }

      tocHeader.addEventListener('click', toggleMenu);
    });

  </script>


  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Google+Sans&family=Noto+Sans&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="preload" href="./static/css/index.css" as="style">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script defer src="./static/js/bulma-carousel.min.js"></script>
  <script defer src="./static/js/bulma-slider.min.js"></script>
  <script defer src="./static/js/index.js"></script>

  <style>
    .video-grid {
      display: grid;
      grid-template-columns: repeat(1, 1fr);
      gap: 1rem;
      width: 100%;
      margin: 0 auto;
      padding: 1rem;
    }

    .video-item {
      background: #fff;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      overflow: hidden;
      transition: transform 0.3s ease, box-shadow 0.3s ease;
      position: relative;
    }

    .video-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2);
    }

    .video-item video {
      width: 100%;
      height: auto;
      display: block;
    }

    .prompt-text {
      position: absolute;
      bottom: 0;
      left: 0;
      right: 0;
      padding: 1rem;
      font-size: 0.9rem;
      line-height: 1.5;
      color: #fff;
      background: linear-gradient(to top, rgba(0, 0, 0, 0.8), rgba(0, 0, 0, 0.6), transparent);
      opacity: 0;
      transform: translateY(100%);
      transition: opacity 0.3s ease, transform 0.3s ease;
    }

    .video-item:hover .prompt-text {
      opacity: 1;
      transform: translateY(0);
    }

    @media (max-width: 1024px) {
      .video-grid {
        grid-template-columns: repeat(2, 1fr);
        gap: 0.75rem;
      }
    }

    @media (max-width: 768px) {
      .video-grid {
        grid-template-columns: 1fr;
        gap: 0.5rem;
        padding: 0.5rem;
      }
    }
  </style>


</head>

<body>


<!--   <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <br>

            <h1 class="title is-1 publication-title">Self Forcing: Bridging Training and Inference in Autoregressive Video Diffusion <br> </h1>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <br>
            <h1 class="title is-1 publication-title" style="margin-bottom:0rem">Compact Attention</h1>
            <h2 class="title is-3 publication-title">Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Qirui Li<sup>1,*</sup>,
                <!-- <a href="https://www.xunhuang.me/">Xun Huang</a><sup>1</sup>, -->
              </span>
              <span class="author-block">
                Guangcong Zheng<sup>1,*</sup>,
                <!-- <a href="https://www.xunhuang.me/">Xun Huang</a><sup>1</sup>, -->
              </span><span class="author-block">
                Qi Zhao<sup>1</sup>,
                <!-- <a href="https://www.xunhuang.me/">Xun Huang</a><sup>1</sup>, -->
              </span><span class="author-block">
                Jie Li<sup>1</sup>,
                <!-- <a href="https://www.xunhuang.me/">Xun Huang</a><sup>1</sup>, -->
              </span><span class="author-block">
                Bin Dong<sup>2</sup>,
                <!-- <a href="https://www.xunhuang.me/">Xun Huang</a><sup>1</sup>, -->
              </span><span class="author-block">
                Yiwu Yao<sup>2</sup>,
                <!-- <a href="https://www.xunhuang.me/">Xun Huang</a><sup>1</sup>, -->
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University,</span>
              <span class="author-block"><sup>2</sup>Huawei Technologies,</span>
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>

            <!-- <div class="is-size-5 column has-text-centered"> -->
              <!-- <strong>CVPR 2025</strong> -->
            <!-- </div> -->
            <!-- button for arxiv and github codes -->
            <!-- <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.08009" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/guandeh17/Self-Forcing"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
              </div>
            </div> -->
            
          </div>
        </div>
      </div>

      <!-- <div class="hero-body" style="padding-bottom: 0rem;">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <div class="content has-text-justified">
                <video controls preload="metadata" autoplay muted loop playsinline style="width: 100%; height: auto; border-radius: 8px;">
                  <source src="static/videos/demo.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
        </div>
      </div> -->

      <div class="hero-body" style="padding-bottom: 0rem;">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h2 class="title is-4" style="color: #3273dc; margin-bottom: 1rem;">TL;DR</h2>
              <div class="content has-text-justified" style="font-size: 1.25rem; line-height: 1.6; background-color: #f8f9fa; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #3273dc;">
                <p>
                  <strong>Compact Attention</strong> accelerates video generation by exploiting structured sparsity in transformer attention. The method identifies patterns that enable efficient computation. Through adaptive tile grouping and automated mask optimization, it eliminates redundant computations, and achieves <strong>2.5×</strong> speedup on models like Hunyuan while maintaining visual quality comparable to full attention. 
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <div class="container " id="overview">
    <!-- <h2 id="obj-comparison" class="title is-3 has-text-centered">Training paradigms for AR video diffusion models</h2> -->
    <div class="content has-text-justified" style="font-size: 1.5rem; line-height: 2.0;">
      <figure class="image is-centered">
        <img src="static/images/firstFig_page.jpg" style="max-width: 73%;">
        <figcaption style="text-align: center; font-size: 1rem; margin-top: 0.5rem; width: 70%; margin: 0 auto;">
        Sparsity between slashes are hard to exploit for acceleration. Periodic, hierarchical attention patterns are shown in complicated attention maps when a single query token is arranged properly.
        </figcaption>
      </figure>
    </div>
  </div>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
               The computational demands of self-attention mechanisms pose a critical challenge for transformer-based video generation, particularly in synthesizing ultra-long sequences. Current approaches, such as factorized attention and fixed sparse patterns, fail to fully exploit the inherent spatio-temporal redundancies in video data. Through systematic analysis of video diffusion transformers (DiT), we uncover a key insight: Attention matrices exhibit structured, yet heterogeneous sparsity patterns, where specialized heads dynamically attend to distinct spatiotemporal regions (e.g., local pattern, cross-shaped pattern, or global pattern). Existing sparse attention methods either impose rigid constraints or introduce significant overhead, limiting their effectiveness. To address this, we propose <strong>Compact Attention</strong>, a hardware-aware acceleration framework featuring three innovations: 1) Adaptive tiling strategies that approximate diverse spatial interaction patterns via dynamic tile grouping, 2) Temporally varying windows that adjust sparsity levels based on frame proximity, and 3) An automated configuration search algorithm that optimizes sparse patterns while preserving critical attention pathways. Our method achieves <strong>1.6~2.5</strong>
               acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines. This work provides a principled approach to unlocking efficient long-form video generation through structured sparsity exploitation.
            </p>
          </div>
        </div>
      </div>
      </div><br>
      <!--/ Abstract. -->
  <!-- </section> -->

  <br>


  <hr>
  <div class="container " id="text2short">
    <h2 id="obj-comparison" class="title is-3 has-text-centered">Faster Video Generation - Acceleration Examples</h2>
    <div class="content has-text-justified" style="font-size: 1.5rem; line-height: 1.8;">
      <p>
        Compact Attention achieves <strong>1.6~2.5x</strong> acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines.
        Below, we show videos accelerated by our method compared to the full attention baseline. The left column shows the original videos generated by DiT with full attention, while the other columns show the videos generated by our method with compact attention with different sparsity.
        <!-- <a href="videos.html">[More Examples]</a> -->
      </p>
    </div>
  </div>
<div>
  <br>

  <div class="container">
    <p class="title is-5 has-text-centered" style="font-size: 1.2rem; line-height: 1.6; color: #4a4a4a;">
      Below are some examples of videos generated by Hunyuan. Attention is speed up by <strong>2.5x</strong> with sparsity of 62.36%. 
    </p>
  </div>
  <br>
  <div class="video-grid">

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/hunyuan/Case11.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">A close up view of a glass sphere that has a zen garden within it. There is a small dwarf in the sphere who is raking the zen garden and creating patterns in the sand.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/hunyuan/Case22.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">Extreme close up of a 24 year old woman's eye blinking, standing in Marrakech during magic hour, cinematic film shot in 70mm, depth of field, vivid colors, cinematic.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/hunyuan/Case33.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/hunyuan/Case44.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">The Glenfinnan Viaduct is a historic railway bridge in Scotland, UK, that crosses over the west highland line between the towns of Mallaig and Fort William. It is a stunning sight as a steam train leaves the bridge, traveling over the arch-covered viaduct. The landscape is dotted with lush greenery and rocky mountains, creating a picturesque backdrop for the train journey. The sky is blue and the sun is shining, making for a beautiful day to explore this majestic spot.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/hunyuan/Case55.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">Animated scene features a close-up of a short fluffy monster kneeling beside a melting red candle. The art style is 3D and realistic, with a focus on lighting and texture. The mood of the painting is one of wonder and curiosity, as the monster gazes at the flame with wide eyes and open mouth. Its pose and expression convey a sense of innocence and playfulness, as if it is exploring the world around it for the first time. The use of warm colors and dramatic lighting further enhances the cozy atmosphere of the image.</p>
    </div>
  </div>

  <div class="container">
    <br>
    <br>
    <p class="title is-5 has-text-centered" style="font-size: 1.2rem; line-height: 1.6; color: #4a4a4a;">
      Below are some examples of videos generated by Wan2.1(14B).Attention is speed up by <strong>1.6x</strong> with sparsity of 62.36%. 
    </p>
  </div>

  <div class="video-grid">
    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/wan2.1/Case11.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">Historical footage of California during the gold rush.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/wan2.1/Case22.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">Extreme close up of a 24 year old woman's eye blinking, standing in Marrakech during magic hour, cinematic film shot in 70mm, depth of field, vivid colors, cinematic.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/wan2.1/Case33.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/wan2.1/Case44.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">A large orange octopus is seen resting on the bottom of the ocean floor, blending in with the sandy and rocky terrain. Its tentacles are spread out around its body, and its eyes are closed. The octopus is unaware of a king crab that is crawling towards it from behind a rock, its claws raised and ready to attack. The crab is brown and spiny, with long legs and antennae. The scene is captured from a wide angle, showing the vastness and depth of the ocean. The water is clear and blue, with rays of sunlight filtering through. The shot is sharp and crisp, with a high dynamic range. The octopus and the crab are in focus, while the background is slightly blurred, creating a depth of field effect.</p>
    </div>

    <div class="video-item">
      <video controls preload="metadata" autoplay muted loop playsinline>
        <source
          src="static/videos/acceleration_examples/wan2.1/Case55.mp4"
          type="video/mp4">
      </video>
      <p class="prompt-text">A white and orange tabby cat is seen happily darting through a dense garden, as if chasing something. Its eyes are wide and happy as it jogs forward, scanning the branches, flowers, and leaves as it walks. The path is narrow as it makes its way between all the plants. the scene is captured from a ground-level angle, following the cat closely, giving a low and intimate perspective. The image is cinematic with warm tones and a grainy texture. The scattered daylight between the leaves and plants above creates a warm contrast, accentuating the cat's orange fur. The shot is clear and sharp, with a shallow depth of field.</p>
    </div>

  </div>


  <br>
  <br>
  <hr>

  <div class="container " id="overview">
    <h2 id="obj-comparison" class="title is-3 has-text-centered">Overview of Compact Attention kernel and off-line Auto-Search of Sparse Masks</h2>
    <div class="content has-text-justified" style="font-size: 1.5rem; line-height: 2.0;">
      <figure class="image is-centered">
        <img src="static/images/pipeline.jpg" style="max-width: 73%;">
        <figcaption style="text-align: center; font-size: 1rem; margin-top: 0.5rem; width: 70%; margin: 0 auto;">
        Compact Attention pipeline. The method identifies structured sparsity patterns in attention maps, enabling efficient computation through adaptive tile grouping and automated mask optimization.
        </figcaption>
      </figure>
    </div>
  </div>

  <hr>
  <div class="container " id="overview">
    <h2 id="obj-comparison" class="title is-3 has-text-centered">Tile-Based Sparsity for Efficient Blockwise Attention</h2>
    <div class="content has-text-justified" style="font-size: 1rem; line-height: 1.8;">
      <p style="max-width: 73%; text-align: justified; margin: 0 auto;">
        Although attention exhibits significant sparsity, token-wise sparse prediction is impractical for real-world acceleration due to high overhead. Experiments reveal that critical attention patterns cluster in 3D space. Processing data block-wise leverages this sparsity clustering, improving efficiency while reducing memory consumption.
        Sparsity validation on Wan1.2 and Hunyuan models compared two flattening strategies: direct 1D flattening vs. spatially grouped 3D tiling. The latter reduces the required blocks for 95% recall by ​​1.1%​​ (Wan2.1) to ​​3.4%​​ (Hunyuan), maintaining high sparsity and compatibility with block-wise attention. This approach optimizes acceleration while preserving model performance.
      </p>
    </div>

    <div class="content has-text-justified" style="font-size: 1.5rem; line-height: 2.0; ">
      <figure class="image is-centered" >
        <img src="static/images/topk_sparsity.jpg" style="width: 73%;">
        <figcaption style="text-align: center; font-size: 1rem; margin-top: 0.5rem; width: 70%; margin: 0 auto;">
        Heatmap of attention map and the k values required to retain top-k for 0.95 recall before and after rearranging attention maps into 3D spatially adjacent groups.
        </figcaption>
      </figure>
    </div>
  </div>
  <br>

  <hr>
  <div class="container " id="overview">
    <h2 id="obj-comparison" class="title is-3 has-text-centered">Structured Spatiotemporal Patterns in Attention Maps</h2>
    <div class="content has-text-justified" style="font-size: 1rem; line-height: 1.8;">
      <p style="max-width: 73%; text-align: justified; margin: 0 auto;">
        Attention maps from 1D sequences with 3D structure (f, h, w) show complex morphological diversity. Prior work in video generation models has identified clustered patterns (e.g., slash-lines, vertical-lines, blocks) and designed sparse primitives to approximate them. However, fine-grained per-query analysis reveals that diagonal patterns stem from systematic positional biases, forming distinct spatiotemporal modes. Empirical studies identify ​​three dominant spatial​ and ​​two temporal patterns​​ common in 3D full-attention video models
      </p>
    </div>

    <div class="content has-text-justified" style="font-size: 1.5rem; line-height: 2.0;">
      <figure class="image is-centered">
        <img src="static/images/DifferentAttentionPattern.png" style="width: 73%;">
        <figcaption style="text-align: center; font-size: 1rem; margin-top: 0.5rem; width: 70%; margin: 0 auto;">
        Three characteristic attention patterns observed in video transformers: local pattern (left), cross-shaped pattern(middle) and global pattern(right) with the upper one showing temporal dynamics while lower one being persistent along frames. Each Attention map is shown using query of the token in the middle and keys from the other tokens.
        </figcaption>
      </figure>
    </div>
  </div>

  <hr>
  <div class="container " id="overview">
    <h2 id="obj-comparison" class="title is-3 has-text-centered">Pattern Stability Enables Offline Acceleration</h2>
    <div class="content has-text-justified" style="font-size: 1rem; line-height: 1.8;">
      <p style="max-width: 73%; text-align: justified; margin: 0 auto;">
        Our analysis shows that spatiotemporal attention patterns are intrinsic to the model architecture, not input-dependent. These patterns remain stable across layers and heads. We classify them as ​​global​​ (covering >85% spatial extent) or ​​local​​ (focused interaction regions), enabling offline acceleration through predictable sparse computation.
      </p>
    </div>

    <div class="content has-text-justified" style="font-size: 1.5rem; line-height: 2.0;">
      <figure class="image" style="display: flex; flex-direction: column; align-items: center;">
    <div style="display: flex; justify-content: center; gap: 5%; width: 100%;">
        <img src="static/images/calSize_new.jpg" style="width: 30%;">
        <img src="static/images/pattern_sim_new.jpg" style="width: 30%;">
    </div>
    <figcaption style="text-align: center; font-size: 1rem; margin-top: 0.5rem; width: 70%;">
        (a) A visualization of the layer-wise computation during denoising. For different prompts, the computational demands are nearly identical.<br>
        (b) Region similarity across denoising steps.
    </figcaption>
</figure>
    </div>
  </div>
  <br>
  <hr>

  <div class="container ">
    <h2 id="obj-comparison" class="title is-3 has-text-centered">BibTeX</h2>
  </div>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div style="position: relative;">
        <pre><code id="bibtex-code">@article{huang2025selfforcing,
  title={Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion},
  author={Huang, Xun and Li, Zhengqi and He, Guande and Zhou, Mingyuan and Shechtman, Eli},
  journal={arXiv preprint arXiv:2506.08009},
  year={2025}
}</code></pre>
        <button id="copy-bibtex" onclick="copyBibtex()" style="position: absolute; top: 10px; right: 10px; background: #3273dc; color: white; border: none; padding: 8px 12px; border-radius: 4px; cursor: pointer; font-size: 12px;">
          Copy
        </button>
      </div>
    </div>
  </section>

  <script>
    function copyBibtex() {
      const bibtexText = `@article{huang2025selfforcing,
  title={Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion},
  author={Huang, Xun and Li, Zhengqi and He, Guande and Zhou, Mingyuan and Shechtman, Eli},
  journal={arXiv preprint arXiv:2506.08009},
  year={2025}
}`;

      navigator.clipboard.writeText(bibtexText).then(function() {
        const button = document.getElementById('copy-bibtex');
        const originalText = button.textContent;
        button.textContent = 'Copied!';
        button.style.background = '#48c774';
        setTimeout(function() {
          button.textContent = originalText;
          button.style.background = '#3273dc';
        }, 2000);
      }).catch(function(err) {
        console.error('Failed to copy text: ', err);
      });
    }
  </script>

  <footer class="footer">
    <div class="container" align="center">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>



  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var videos = document.querySelectorAll('#results-carousel video');

      videos.forEach(function (video) {
        var isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

        if (!isMobile) {
          // console.log('Autoplay enabled for non-mobile devices');
          if (!video.classList.contains('single_image_video')) {
            video.setAttribute('autoplay', 'autoplay');
            // video.play().catch(function(error) {
            //     console.error('Video playback failed:', error);
            // });
          }

          // else case is implicit - no action needed for 'single_image_video'
        } else {
          console.log('Autoplay disabled for mobile devices');
        }
      });
    });
  </script>

</body>

</html>